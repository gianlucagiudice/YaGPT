# YaGPT: GPT Implementation from Scratch Using PyTorch

![YaGPT Text Generation](assets/generation_sample.png)  <!-- Placeholder for an image showing the model generating text -->

This repository presents YaGPT (Yet Another GPT), a GPT (Generative Pre-trained Transformer) model implemented entirely from scratch using only PyTorch. The model is trained exclusively on the Divina Commedia, aiming to generate text in the style of Dante Alighieri’s masterpiece. Note that only the pre-training phase has been completed, and no fine-tuning has been performed.

## 🌐 Introduction to Transformer Architecture

Introduced by Vaswani et al. in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762), the transformer architecture revolutionized the field of natural language processing with its self-attention mechanism. Unlike traditional RNNs, transformers can process data sequences in parallel, making them highly efficient for modern hardware.

## 🔍 GPT and the Transformer Decoder

GPT (Generative Pre-trained Transformer) is an autoregressive language model that uses the decoder part of the transformer architecture to generate coherent and contextually relevant text. The transformer architecture consists of two main components:

1. **Encoder**: Processes the input sequence and generates a context-aware representation.
2. **Decoder**: Uses the encoder's output to generate the target sequence one token at a time.

In a GPT model, only the decoder component is used. This allows the model to generate text by predicting the next token in a sequence based on the previously generated tokens.

## 🏗️ Model Architecture

YaGPT follows the standard transformer decoder architecture with the following key components:

1. **Tokenization**: Text is tokenized into individual words or subwords.
2. **Embedding Layer**: Tokens are converted into dense vector representations.
3. **Positional Encodings**: Encodings are added to embeddings to retain positional information.
4. **Transformer Decoder Blocks**: A series of transformer layers with self-attention mechanisms and feed-forward networks.
5. **Output Layer**: Generates probabilities for the next token in the sequence.

## 🔧 Model Hyperparameters

Understanding the hyperparameters used in YaGPT is crucial for reproducing and fine-tuning the model:

- **seq_len**: Maximum sequence length for the input text.
- **d_model**: Dimensionality of the token embeddings.
- **n_heads**: Number of attention heads in each transformer layer.
- **n_layers**: Number of transformer layers in the decoder.
- **d_ff**: Dimensionality of the feed-forward network within the transformer.
- **dropout**: Dropout rate for regularization.
- **vocab_size**: Size of the vocabulary derived from the training dataset.

## ✍ YaGPT: Generating the Divina Commedia

YaGPT has been trained from scratch on the Divina Commedia to capture the unique style, language, and structure of this literary work. Below is a breakdown of the implementation and training process.

### 🛠️ Training and Hyperparameters

To train YaGPT, follow these steps:

1. Navigate to the repository's root directory.
2. Execute the training script:

   ```
   python scripts/train.py --dataset_path [DATASET_PATH] --batch_size [BATCH_SIZE] --d_model [D_MODEL] --seq_len [SEQ_LEN] --n_heads [N_HEADS] --n_layers [N_LAYERS] --dropout [DROPOUT] --max_epochs [MAX_EPOCHS] --lr [LEARNING_RATE]
   ```

## 📊 Performance and Results

### Overview

YaGPT has been evaluated based on its ability to generate text that mimics the style and content of the Divina Commedia. The generated text is assessed qualitatively for coherence, fluency, and adherence to the original text’s style.

### Example Output

Here is a sample output generated by YaGPT after training on the Divina Commedia:

```
>>> Input prompt <<<
Inferno
Canto XXXIV

Giunti là dove 
>>> Generated text <<<
'l duca mio li s'accostò allato;
domandollo ond' ei fosse, e quei rispuose:
<<I' fui del regno di Navarra nato.

Mia madre a servo d'un segnor mi puose,
che m'avea generato d'un ribaldo,
distruggitor di sé e di sue cose.

Poi fui famiglia del buon re Tebaldo;
quivi mi misi a far baratteria,
di ch'io rendo ragione in questo caldo>>.
```
